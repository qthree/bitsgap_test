# 2025-02-04

- в ТЗ: `Начинаем получать RT (на бирже Trades) по WS и собирать новые KL`
    - Получать KL по WS или вычислять их самому на основе RT?

- в ТЗ: `Список таймфреймов – 1м, 15м, 1ч, 1д`
    - Хранить в базе KL для каждого интервала?
    - Или хранить только для 1м, а для других вычислять на лету в запросе к БД? Если БД такое позволяет.
    - Или хранить вообще только RT и на основе их вычислять какждый раз?
    - Лучшее решение зависит от того как часто будут запросы к 1ч-1д KL, если часто то лучше хранить их
        - Чтобы каждый раз заново не рассчитывать
        - Всё равно место в БД 1ч-1д будут занимать сильно меньше чем 1м KL
        - Сохраняем 1ч-1д KL, можно оптимизировать потом.
    - Так же завист от количества RT в минуту
        - Если часто будут минуты когда не было ни одной сделки - будет сохраняться много "пустых" KL
            - Надо ли сохранять эти пустые KL?
            - Сохраняем пустые KL, можно оптимизировать потом.
        - Если будет слишком много запросов в минуту, то даже 1м KL будет дорого рассчитывать
            - Сохраняем 1м-15м, можно будет оптимизировать потом.
    - В тестовом проекте не идёт речь о запросах данных из БД, только о наполнении её данными.
        - В любом случае, пока заполняем БД всеми пришедшими данными
        - т.к. единственный способ проверить корректность данных пока - с помощью сторонних инструментов БД
            - и тестов?
            - Было бы неплохо написать тесты для проверки содержимого БД, хоть чтение из БД в проект не входит
            - Получается придётся в любом случае добавить минимальную реализацию чтения из БД?

- В интервью просили сделать решение "масштабируемым"
    - Если речь о производительности
        - Rust и Tokio уже предоставляют удовлитворительную производительность, а точнее минимальный overhead
        - Предусмотреть использование БД которую можно масштабировать
            - Количество операций записи не будет зависеть от количества пользователей агрегатора
                - Не требуется возможность писать в разные инстансы БД
                - Не требуется алгоритм консенсуса
                - Достаточно чтобы данные текли от основного инстанса в зеркала
            - Нужно изучить характер данных
                - Возможно в реальном проекте вместо масштабирования БД можно будет обойтись (частично?) кэшированием данных in-memory приложения
            - В интервью спрашивали про знания о Tarantool
                - Есть планы использовать его в реальном проекте?
                - Можно попробовать использовать его
            - Предусмотреть возможность использовать разных БД в качестве бэкенда для хранения данных?
                - Можно отрефакторить потом при надобности
                - Слишком out-of-scope
                - Трудно заранее сформулировать подходящий уровень абстракции для абсолютно разных БД (MongoDB vs PostreSQL, например)
        - В тестовом проекте не идёт речь о запросах данных из БД, только о наполнении её данными
            - Производтельность получение данных от API бирж никак не зависит от количества пользователей агрегатора, и в целом не является проблемой.
            - Заботиться о производительности придётся только при имплементации REST API агрегатора и чтения из БД
    - Если речь о использовании API разных бирж
        - Предусмотреть струтктуру проекта которая позволяет подключать новые биржи
            - с наименьшим boilerplate
            - при этом инкапсулировать переиспользуемые код и код для разных бирж

- Структура проекта
    - Библиотека с общим кодом
        - В общей библиотеке должны быть универсальные типы которые сохраняются в БД, с сигнатурой как в ТЗ
        - Можно принить к этим универсальным типам derive serde::Serialize, если нужна сериализация в JSON
            - Eсли будем отдавать их по своему REST API в полноценном проекте
            - Если будем сохранять как json в БД (MongoDB?)
    - По отдельной библиотеке на каждую биржу, зависят от общей библиотеки
        - В отдельных библиотеках живут типы специфичные для соответствующего REST API
        - Применяем к этим специфичным типам derive serde::Deserialize чтобы парсить ответ от REST API
        - Можно конвертировать специфичные типы в универсальный с помощью моего крейта derive_convert
    - Исполняемый крейт для получения и сохранения данных
        - Включает в себя крейты всех бирж, который можно включать/выключать при компиляции (features) и в рантайме (конфиг)
    - В будущем возможны другие исполняемые крейты которые будут выполнять функции (чтение из БД, торговые операции)
        - которые смогут переиспользовать те же библиотеки с общим кодом и с кодом специфичным для бирж

- Типы данных
    - В реальном проекте стоит обсудить использование внутренних типов вместо String
    - Строгая типизация поможет избежать логических ошибок
        - Сделает невозможным хранить в памяти невалидный стейт
        - После первичного парсинга мы будем уверены что данная запись всегда валидна
    - Нужно понять какие типы должны иметь возможность меняться/дополняться без перекомпиляции
    - `struct RecentTrade`
        - `tid: String` -> тип гарантирующий валидный формат Id
        - `pair: String` -> `(String, String)`, опционально с обёрткой, которая гарантирует что мы поддерживаем такую валютную пару
        - `price: String` и `amount: String` -> число не с плавающей точкой
            - см. crate [`fastnum`](https://docs.rs/fastnum/latest/fastnum/)
            - или [`Ratio<i128>`](https://docs.rs/num/latest/num/rational/struct.Ratio.html) в `num`
        - `side: String` -> `enum Side { Buy, Sell }`
        - `timestamp: i64` -> обёртка с методами для конвертации в типы данных из std/time/chrono
    - `struct Kline`
        - `time_frame: String`
            - Фиксированные значения -> `enum TimeFrame { Minute, Minute15, Hour, Day }`
            - Динамическое значение (мы не знаем какие могут быть TF на будущих биржах) -> аналог `Duration`
        - `o, h, l, c, volume_bs`: `f64` -> число не с плавающей точкой?
            - или лучше прокинуть насквозь как есть, чтобы избежать потерь при конверсии туда-обратно?
    - При сохранении в БД можно сохранять по прежнему как String
        - т.е. эти изменения никак не коснутся результата в БД, только организацию внутри программы
    - в ТЗ указаны конкретные сигнатуры типов, оставим пока как есть, без парсинга String, потом можно отрефакторить.

- Обработка ошибок
    - В библиотеках желательно использовать thiserror
    - Но для тестового прототипа пойдёт и anyhow
    - Позже можно отрефакторить, когда будут известны основные типы ошибок.

- Инструменты в shared крейте
    - `ApiRequester`
        - Отправка GET запросов и парсинг json ответа
        - Аутентификация с `enum AuthMethod`
            - Позволит испольховать на разных биржах схожие способы аутентификации без дублирования кода
            - На poloniex.com используется HMAC-SHA256
            - В хэдеры каждого запроса помещается sign который генерируется из параметров запроса
                - От timestamp запроса он не зависит
                - Можно было бы сгенерировать его всего один раз для запроса с неизменным url и параметрами
                - Но оно того вряд ли стоит
            - Наивная имплементация подразумевает что если на других биржах тоже есть HMAC-SHA256 auth, то там используются точно такие же хэдеры
                - Разумеется это не так, но пока сложно выбрать подходящую абстракцию, чтобы поддержать HMAC-SHA256 с разными названиями и характеристиками хэдеров
                - Возможно придётся делать отдельный набор `AuthMethod` для каждой биржи, и делать инкапсуляцию на уровне функций, которые уже вызывать из методов `AuthMethod`
                - Потребуется рефакторинг.

- Настройка
    - Возможно, если делать внешний конфиг, например в формате toml, нужно разделить "биржу" на две сущности
        - "API"
            - credentials
            - выбор какой модуль отвечает за парсинг ответов, конвертацию их в универсальные
            - не знает ничего о сборе RT и KL
        - "Scrapper"
            - непосредственно настройка сборщика RT и KL
            - указывает на один из "API"
        - Возможны произвольные комбинации API и Scrapper
            - Несколько Scrapper используют один и тот же API, но с разными настройками, например разные пары, или разные промежутки KL.
            - Несколько API для одной и той же биржи, по одному Scrapper (или по одному торговому боту) на каждый, чтобы уменьшить количество обращений по одному ключу.
                - В реальном проекте же у каждого клиента будут свои credentials
            - Возможно, один scrapper и множество API у него? Чтобы переиспользовать настройки одного scrapper через множество API?
    - Предусмотреть, что в реальном проекте, при настройке торговых ботов, потребуется реалтайм добавление/удаление конфигов (API, боты)

- За первый день успел сделать
    - Отправка GET запроса по произвольному пути к API poloniex
    - Формирование auth payload, подписание приватным ключом, добавление в хэдеры запроса
    - Получение api и secret key из ENV VAR
    - Тест в shared крейте который делает запрос к ручке "/markets" и печатает pretty printed json value
    - Сработало с первого запуска
```json
[
  {
    "baseCurrencyName": "BTS",
    "crossMargin": {
      "maxLeverage": 1,
      "supportCrossMargin": false
    },
    "displayName": "BTS/BTC",
    "quoteCurrencyName": "BTC",
    "state": "NORMAL",
    "symbol": "BTS_BTC",
    "symbolTradeLimit": {
      "amountScale": 8,
      "highestBid": "0",
      "lowestAsk": "0",
      "minAmount": "0.00001",
      "minQuantity": "100",
      "priceScale": 10,
      "quantityScale": 0,
      "symbol": "BTS_BTC"
    },
    "tradableStartTime": 1659018816626,
    "visibleStartTime": 1659018816626
  },
  ...
]
```

# 2025-02-05

## TODO
- Запрос KL и RT по REST API
- Написать или [сгенерировать](https://transform.tools/json-to-rust-serde) специфичные для poloniex типы, для парсинга ответа REST ручек.
- Скопировать из ТЗ универсальные типы, добавить конверсию из специфичных типов
- Заимплементить получение данных по WS
    - в ТЗ говорится что мы сначало получаем данные по REST, а потом по WS
    - возможно стоит делать в обратном порядке?
    - нужно убедиться что между REST запросом и подпиской на WS мы не пропустим события RT
    - если мы сначало сделаем подписку на WS, то можем получить дублирующиеся RT, но это решить проще
    - Нужно проверить на реальных данных.

## Recent trades
- У ручки нет указания временного промежутка, только последние N событий
- Что в принципе логично, не зря это RECENT trades
- `Интервал времени – 2024-12-01 – текущее время`
    - Не правильно понял ТЗ?
    - Видимо это только для KL
- `Начинаем получать RT (на бирже Trades) по WS`
    - Ну да, по REST значит получаем только KL, а RT только по WS
    - Имплементацию WS отложу на потом

## Candles (KL)
- Нужно собирать для `BTC_USDT, TRX_USDT, ETH_USDT, DOGE_USDT, BCH_USDT`
    - названия из ТЗ совпдают с "symbol" в выводе "/markets"
    - значит названия из ТЗ не нужно преобразовывать, как есть скармливать запросу
- Жалко что нельзя запрашивать сразу для нескольких symdol в одном запросе
- Насколько позволительно запросить все symbolы параллельно?
    - Последовательно будет медленно
    - Параллельно может упереться в rate limit
        - У `/markets/{symbol}/candles` ограниченте на 200 запросов в секунду
    - Встроить в ApiRequester ограничитель количества одновременных запросов?
        - Или количтва запросов в секунду?
        - Но ApiRequester это низкоуровневая абстракция которая не знает у какой ручки какой rate limit
    - Мы делаем запросы к candles только при старте программы, и всего 5, по запросу на symbol
    - Идеальное решение сейчас трудно сформулировать, для тестового проекта не является проблемой
    - Отложить на потом, пока можно делать запросы параллельно без rate limit.
- Интервалы
    - Снова встаёт вопрос: нужна ли возможность обновлять API биржи без пересборки кода
        - Например, добавлять/менять доступные для данной биржи интервалы

# WIP
- Вместо того чтобы делать ТЗ увлёкся вспомогательным кодом
    - trait BuildUrl для эффективного и удобного создания запросов
        - можно было бы использовать частично serde крейты для работы с url
            - где то было бы более удобно, но не так гибко
        - но например если нужен некий контекст для сериализации то serde не подойдёт
            - словарь для конвертации параметров запроса между разными биржами, например
        - нужно будет подумать когда наберётся больше примеров использования
        - заменять ли на что-то, или дальше развивать
            - например, свой derive написать
            - или использовать тот что уже [есть](https://github.com/qthree/represent)
        - Пока оставлю.
    - SortedVec чтобы хранить динамический словарь названий интервалов
    - Надо переходить ближе к делу.

- ручка `candles` работает (пока без парсинга ответа)
```json
[
  [
    "98113.42",
    "98177.78",
    "98171.66",
    "98113.42",
    "77317.88",
    "0.787699",
    "34974.58",
    "0.356324",
    88,
    1738770236995,
    "98156.63",
    "MINUTE_1",
    1738770180000,
    1738770239999
  ],
  ...
]
```
