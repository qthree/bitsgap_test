- `Начинаем получать RT (на бирже Trades) по WS и собирать новые KL`
    - Получать KL по WS или вычислять их самому на основе RT?

- `Список таймфреймов – 1м, 15м, 1ч, 1д`
    - Хранить в базе KL для каждого интервала?
    - Или хранить только для 1м, а для других вычислять на лету в запросе к БД? Если БД такое позволяет.
    - Или хранить вообще только RT и на основе их вычислять какждый раз?
    - Лучшее решение зависит от того как часто будут запросы к 1ч-1д KL, если часто то лучше хранить их
        - Чтобы каждый раз заново не рассчитывать
        - Всё равно место в БД 1ч-1д будут занимать сильно меньше чем 1м KL
        - Сохраняем 1ч-1д KL, можно оптимизировать потом.
    - Так же завист от количества RT в минуту
        - Если часто будут минуты когда не было ни одной сделки - будет сохраняться много "пустых" KL
            - Надо ли сохранять эти пустые KL?
            - Сохраняем пустые KL, можно оптимизировать потом.
        - Если будет слишком много запросов в минуту, то даже 1м KL будет дорого рассчитывать
            - Сохраняем 1м-15м, можно будет оптимизировать потом.
    - В тестовом проекте не идёт речь о запросах данных из БД, только о наполнении её данными.
        - В любом случае, пока заполняем БД всеми пришедшими данными
        - т.к. единственный способ проверить корректность данных пока - с помощью сторонних инструментов БД
            - и тестов?
            - Было бы неплохо написать тесты для проверки содержимого БД, хоть чтение из БД в проект не входит
            - Получается придётся в любом случае добавить минимальную реализацию чтения из БД?

- В интервью просили сделать решение "масштабируемым"
    - Если речь о производительности
        - Rust и Tokio уже предоставляют удовлитворительную производительность, а точнее минимальный overhead
        - Предусмотреть использование БД которую можно масштабировать
            - Количество операций записи не будет зависеть от количества пользователей агрегатора
                - Не требуется возможность писать в разные инстансы БД
                - Не требуется алгоритм консенсуса
                - Достаточно чтобы данные текли от основного инстанса в зеркала
            - Нужно изучить характер данных
                - Возможно в реальном проекте вместо масштабирования БД можно будет обойтись (частично?) кэшированием данных in-memory приложения
            - В интервью спрашивали про знания о Tarantool
                - Есть планы использовать его в реальном проекте?
                - Можно попробовать использовать его
            - Предусмотреть возможность использовать разных БД в качестве бэкенда для хранения данных?
                - Можно отрефакторить потом при надобности
                - Слишком out-of-scope
                - Трудно заранее сформулировать подходящий уровень абстракции для абсолютно разных БД (MongoDB vs PostreSQL, например)
        - В тестовом проекте не идёт речь о запросах данных из БД, только о наполнении её данными
            - Производтельность получение данных от API бирж никак не зависит от количества пользователей агрегатора, и в целом не является проблемой.
            - Заботиться о производительности придётся только при имплементации REST API агрегатора и чтения из БД
    - Если речь о использовании API разных бирж
        - Предусмотреть струтктуру проекта которая позволяет подключать новые биржи
            - с наименьшим boilerplate
            - при этом инкапсулировать переиспользуемые код и код для разных бирж

- Структура проекта
    - Библиотека с общим кодом
        - В общей библиотеке должны быть универсальные типы которые сохраняются в БД, с сигнатурой как в ТЗ
        - Можно принить к этим универсальным типам derive serde::Serialize, если нужна сериализация в JSON
            - Eсли будем отдавать их по своему REST API в полноценном проекте
            - Если будем сохранять как json в БД (MongoDB?)
    - По отдельной библиотеке на каждую биржу, зависят от общей библиотеки
        - В отдельных библиотеках живут типы специфичные для соответствующего REST API
        - Применяем к этим специфичным типам derive serde::Deserialize чтобы парсить ответ от REST API
        - Можно конвертировать специфичные типы в универсальный с помощью моего крейта derive_convert
    - Исполняемый крейт для получения и сохранения данных
        - Включает в себя крейты всех бирж, который можно включать/выключать при компиляции (features) и в рантайме (конфиг)
    - В будущем возможны другие исполняемые крейты которые будут выполнять функции (чтение из БД, торговые операции)
        - которые смогут переиспользовать те же библиотеки с общим кодом и с кодом специфичным для бирж
